# Model Server and Metrics Demo

Welcome to the Model Server and Metrics demo! This repository showcases key capabilities of the Red Hat OpenShift AI platformâ€”specifically around deploying and monitoring machine learning (ML) models at scale using KServe.

## ğŸ“Œ Overview

This demo highlights the work of the Red Hat OpenShift AI Model Server and Metrics team. Our goal is to:

- Deploy and monitor containerized ML models across hybrid cloud environments.
- Enable real-time observability and ensure performance and reliability of models.
- Provide end-to-end ML lifecycle management via scalable and modular serving infrastructure.

## ğŸ¯ Demo Goals

- Show real-world use cases of serving and monitoring models on OpenShift AI.
- Demonstrate specialized features like transformers, autoscaling, and multi-node LLM support.
- Highlight tools that improve observability, performance, and reliability.

---

## ğŸ¶ Initial Demo: Dog Breed Classification

### ğŸ” Description
We deploy a lightweight image classification model that detects dog breeds from images using OpenShift AI and KServe.

### ğŸ“Œ Capabilities Demonstrated

- **Basic inference service**
- **Transformer Integration:** Clean and normalize input data before inference.
- **Autoscaling (Scale to 0):** Efficient resource use with demand-based scaling.
- **Stop/Resume Annotations:** Pause/resume models for experimental workflows.
- **Metrics & Dashboards:** Real-time insight into model health and performance.

### ğŸ”— Demo Steps
- See: [Basic demo steps](https://github.com/hdefazio/modelservermetrics_demo/tree/main/basic_demo)
---

## ğŸ§  Multi-Node LLM Demo

### ğŸ“Œ Problem
Large language models (LLMs) often exceed the memory limits of a single node.

### ğŸ’¡ Solution
Multi-node support allows you to split a single model inference across several nodes in the cluster.


### ğŸ”— Demo Steps
See: [Multi-node demo steps](https://github.com/hdefazio/modelservermetrics_demo/tree/main/multi_node)

### âœ… Highlights

- Horizontally scales LLM deployments across the cluster.
- No longer constrained by single-node GPU memory.

---

## ğŸ”— Inference Graph Demo

### ğŸ“Œ Concept
Chain multiple models together so the output of one becomes the input of the nextâ€”all behind a single endpoint.

Example:  
Dog breed â†’ LLM poem generation about that dog â†’ Combined response


### ğŸ”— Demo Steps
See: [Inference Graph demo steps](https://github.com/hdefazio/modelservermetrics_demo/tree/main/inference_graph)

### âœ… Highlights

- Simplifies application logic with a declarative graph managed by KServe.
- Provides a unified API endpoint for complex model workflows.
- Efficient resource useâ€”each `InferenceService` can scale independently.

---
---

## ğŸ™Œ Contributors

- Hannah DeFazio â€“ Red Hat OpenShift AI
- Mariah Holder â€“ Red Hat OpenShift AI
